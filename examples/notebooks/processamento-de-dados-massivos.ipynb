{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "# PostgreSQL to S3 Export using Glue/Spark",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "import sys\nimport boto3\nimport json\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Initialize Glue context\nsc = SparkContext()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "ValueError: Cannot run multiple SparkContexts at once; existing SparkContext(app=GlueReplApp, master=jes) created by __init__ at /tmp/4543972804912652780:514 \n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Get credentials from Secrets Manager\nsecrets_client = boto3.client('secretsmanager')\nsecret = secrets_client.get_secret_value(SecretId='rds-read-only')\ncredentials = json.loads(secret['SecretString'])",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# JDBC connection properties using Secrets Manager\njdbc_url = f\"jdbc:postgresql://{credentials['host']}:{credentials['port']}/{credentials['dbname']}\"\nconnection_properties = {\n    \"user\": credentials['username'],\n    \"password\": credentials['password'],\n    \"driver\": \"org.postgresql.Driver\"\n}",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 7,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "def print_tables_with_index():\n    query = \"\"\"\n    SELECT table_name \n    FROM information_schema.tables \n    WHERE table_schema = 'public' AND table_type = 'BASE TABLE'\n    ORDER BY table_name\n    \"\"\"\n    \n    tables_df = spark.read \\\n        .format(\"jdbc\") \\\n        .option(\"url\", jdbc_url) \\\n        .option(\"query\", query) \\\n        .option(\"user\", credentials['username']) \\\n        .option(\"password\", credentials['password']) \\\n        .option(\"driver\", \"org.postgresql.Driver\") \\\n        .load()\n    \n    all_tables = [row.table_name for row in tables_df.collect()]\n    valid_tables = []\n    \n    for table in all_tables:\n        try:\n            spark.read \\\n                .format(\"jdbc\") \\\n                .option(\"url\", jdbc_url) \\\n                .option(\"query\", f\"SELECT 1 FROM {table} LIMIT 1\") \\\n                .option(\"user\", credentials['username']) \\\n                .option(\"password\", credentials['password']) \\\n                .option(\"driver\", \"org.postgresql.Driver\") \\\n                .load().collect()\n            valid_tables.append(table)\n        except:\n            pass\n    \n    print(\"# Valid tables:\")\n    for i, table in enumerate(valid_tables):\n        padding = max(0, 40 - len(table))\n        print(f'    \"{table}\",{\" \" * padding}# {i}')\n    \n    return valid_tables\n\ntables = print_tables_with_index()\n\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 8,
			"outputs": [
				{
					"name": "stdout",
					"text": "# Valid tables:\n    \"censo_escolar_2024\",                      # 0\n    \"ed_enem_2024_participantes\",              # 1\n    \"ed_enem_2024_resultados\",                 # 2\n    \"ed_enem_2024_resultados_amos_per\",        # 3\n    \"ed_superior_cursos\",                      # 4\n    \"ed_superior_ies\",                         # 5\n    \"educacao_basica\",                         # 6\n    \"municipio\",                               # 7\n    \"municipio_ride_brasilia\",                 # 8\n    \"ocorrencia\",                              # 9\n    \"pib_municipios\",                          # 10\n    \"regiao\",                                  # 11\n    \"sus_aih\",                                 # 12\n    \"sus_procedimento_ambulatorial\",           # 13\n    \"unidade_federacao\",                       # 14\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "table_name = tables[0]\nprint(\"tbl:\", table_name)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 9,
			"outputs": [
				{
					"name": "stdout",
					"text": "tbl: censo_escolar_2024\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Usage with selected table\ndf = spark.read.jdbc(\n        url=jdbc_url,\n        table=table_name,\n        properties=connection_properties\n    )\n\ndf.sample(.1).toPandas().head()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 28,
			"outputs": [
				{
					"name": "stdout",
					"text": "   NU_ANO_CENSO NO_REGIAO  ...  QT_TUR_FUND_AF_INT QT_TUR_MED_INT\n0          2024   Sudeste  ...                 NaN            NaN\n1          2024   Sudeste  ...                 NaN            NaN\n2          2024   Sudeste  ...                 0.0            0.0\n3          2024   Sudeste  ...                 0.0            0.0\n4          2024       Sul  ...                 0.0            0.0\n\n[5 rows x 426 columns]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df.write.mode(\"overwrite\").parquet(f\"s3://iesb-bigdata/bronze/{table_name}/\")\n        \nprint(f\"Table {table_name}\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 29,
			"outputs": [
				{
					"name": "stdout",
					"text": "Table censo_escolar_2024\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "for table_name in tables:\n    df = spark.read.jdbc(\n        url=jdbc_url,\n        table=table_name,\n        properties=connection_properties\n    )\n\n    df.write.mode(\"overwrite\").parquet(f\"s3://iesb-bigdata/bronze/{table_name}/\")\n    print(f\"Table {table_name}\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		}
	]
}