{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bronze Layer Creation - IESB BigData Class\n",
    "\n",
    "This notebook creates a bronze layer from PostgreSQL tables with proper organization and metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import boto3\n",
    "import json\n",
    "from datetime import datetime\n",
    "from awsglue.transforms import *\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from pyspark.sql.functions import current_timestamp, lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize contexts\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "BUCKET = \"iesb-bigdata\"\n",
    "BRONZE_PATH = f\"s3://{BUCKET}/bronze\"\n",
    "BATCH_DATE = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "print(f\"Bronze Layer Path: {BRONZE_PATH}\")\n",
    "print(f\"Batch Date: {BATCH_DATE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get RDS credentials\n",
    "secrets_client = boto3.client('secretsmanager')\n",
    "secret = secrets_client.get_secret_value(SecretId='rds-secret')\n",
    "credentials = json.loads(secret['SecretString'])\n",
    "\n",
    "jdbc_url = f\"jdbc:postgresql://{credentials['host']}:{credentials['port']}/{credentials['db_name']}\"\n",
    "connection_properties = {\n",
    "    \"user\": credentials['username'],\n",
    "    \"password\": credentials['password'],\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "print(\"✓ Database connection configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organized table categories\n",
    "tables_config = {\n",
    "    \"geographic\": [\"municipio\", \"unidade_federacao\", \"regiao\", \"municipio_ride_brasilia\"],\n",
    "    \"education\": [\"ed_enem_2024_resultados\", \"ed_enem_2024_participantes\", \"educacao_basica\", \n",
    "                  \"censo_escolar_2024\", \"ed_superior_cursos\", \"ed_superior_ies\"],\n",
    "    \"health\": [\"sus_aih\", \"sus_procedimento_ambulatorial\"],\n",
    "    \"demographics\": [\"Censo_20222_Populacao_Idade_Sexo\", \"agregados_setores_censitarios\"],\n",
    "    \"economics\": [\"pib_municipios\"],\n",
    "    \"incidents\": [\"ocorrencia\"]\n",
    "}\n",
    "\n",
    "# Show organization\n",
    "for category, tables in tables_config.items():\n",
    "    print(f\"{category.upper()}: {len(tables)} tables\")\n",
    "    for table in tables:\n",
    "        print(f\"  - {table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process one category as example (Geographic)\n",
    "category = \"geographic\"\n",
    "tables = tables_config[category]\n",
    "\n",
    "print(f\"Processing {category.upper()} tables...\")\n",
    "\n",
    "for table_name in tables:\n",
    "    print(f\"\\n--- Processing {table_name} ---\")\n",
    "    \n",
    "    # Read from PostgreSQL\n",
    "    df = spark.read.jdbc(\n",
    "        url=jdbc_url,\n",
    "        table=table_name,\n",
    "        properties=connection_properties\n",
    "    )\n",
    "    \n",
    "    # Show schema and sample\n",
    "    print(f\"Schema for {table_name}:\")\n",
    "    df.printSchema()\n",
    "    \n",
    "    print(f\"Sample data (first 3 rows):\")\n",
    "    df.show(3)\n",
    "    \n",
    "    print(f\"Record count: {df.count()}\")\n",
    "    \n",
    "    # Add metadata columns\n",
    "    df_with_metadata = df \\\n",
    "        .withColumn(\"bronze_load_date\", lit(BATCH_DATE)) \\\n",
    "        .withColumn(\"bronze_load_timestamp\", current_timestamp()) \\\n",
    "        .withColumn(\"source_system\", lit(\"postgresql\")) \\\n",
    "        .withColumn(\"table_category\", lit(category))\n",
    "    \n",
    "    # Write to bronze layer\n",
    "    output_path = f\"{BRONZE_PATH}/{category}/{table_name}\"\n",
    "    \n",
    "    df_with_metadata.write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .partitionBy(\"bronze_load_date\") \\\n",
    "        .parquet(output_path)\n",
    "    \n",
    "    print(f\"✓ Saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trigger crawler to update Glue catalog\n",
    "try:\n",
    "    glue_client = boto3.client('glue')\n",
    "    response = glue_client.start_crawler(Name='iesb-s3-crawler')\n",
    "    print(\"✓ S3 Crawler started to update catalog\")\n",
    "    print(\"Check Glue console for crawler status\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠ Could not start crawler: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify bronze layer structure\n",
    "print(\"Bronze Layer Structure:\")\n",
    "print(f\"s3://{BUCKET}/bronze/\")\n",
    "for category in tables_config.keys():\n",
    "    print(f\"├── {category}/\")\n",
    "    for table in tables_config[category]:\n",
    "        print(f\"│   ├── {table}/\")\n",
    "        print(f\"│   │   └── bronze_load_date={BATCH_DATE}/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Glue PySpark",
   "language": "python",
   "name": "glue_pyspark"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
